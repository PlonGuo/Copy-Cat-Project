{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9928e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271abbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model.The first time I saw the new \"The Walking Dead\" trailer, I was so excited. I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # 编码输入文本，添加结束标记\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    # 生成响应\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # 解码并返回响应\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 示例对话\n",
    "user_input = \"Hello, I'm a language model.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've got to find what you love, and that is as true for work as it is for your lovers.The U.S. Department of Justice has filed a lawsuit against the company that owns the National Security Agency's (NSA) Tailored Access Operations (TAO) program, alleging that the program violates the Fourth Amendment.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Check if a CUDA device is available, and if so, move the model to the CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # Encode the input text and add an end-of-sequence token\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # If a CUDA device is available, move the input data to the CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example dialogue\n",
    "user_input = \"You've got to find what you love, and that is as true for work as it is for your lovers.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Prepare Data\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "file_path = \"./Copy-Cat-Project-main/Steve Jobs Stanford Commencement Speech Transcript 2005.txt\"\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=file_path,\n",
    "    block_size=128,\n",
    "    overwrite_cache=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Training Configuration\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.1580843925476074\n",
      "Epoch: 0, Loss: 3.5734238624572754\n",
      "Epoch: 0, Loss: 3.2003633975982666\n",
      "Epoch: 0, Loss: 2.9473118782043457\n",
      "Epoch: 0, Loss: 3.428860902786255\n",
      "Epoch: 0, Loss: 3.517279624938965\n",
      "Epoch: 1, Loss: 2.60526442527771\n",
      "Epoch: 1, Loss: 3.181273937225342\n",
      "Epoch: 1, Loss: 2.845142364501953\n",
      "Epoch: 1, Loss: 2.6161742210388184\n",
      "Epoch: 1, Loss: 3.148547649383545\n",
      "Epoch: 1, Loss: 2.854081392288208\n",
      "Epoch: 2, Loss: 2.3623342514038086\n",
      "Epoch: 2, Loss: 2.8888163566589355\n",
      "Epoch: 2, Loss: 2.6013753414154053\n",
      "Epoch: 2, Loss: 2.3753368854522705\n",
      "Epoch: 2, Loss: 2.860568046569824\n",
      "Epoch: 2, Loss: 2.329719066619873\n"
     ]
    }
   ],
   "source": [
    "# Custom Training Loop\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch[\"input_ids\"].to(device), batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Your prompt text here\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=1000)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Generated Text\n",
    "with open('./results.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "100%|██████████| 112/112 [00:11<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.6419, 'train_samples_per_second': 37.795, 'train_steps_per_second': 9.62, 'train_loss': 2.8110498700823103, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=112, training_loss=2.8110498700823103, metrics={'train_runtime': 11.6419, 'train_samples_per_second': 37.795, 'train_steps_per_second': 9.62, 'train_loss': 2.8110498700823103, 'epoch': 4.0})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#更新后！！！！！\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 准备数据集和数据整理器\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"./Copy-Cat-Project-main/Steve Jobs The Lost Interview 1995.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# # 准备生成文本的初始提示（prompt）\n",
    "# prompt_text = \"Your prompt text here.\"\n",
    "\n",
    "# # 编码输入文本，添加结束标记\n",
    "# input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# # 将模型转移到正确的计算设备（GPU或CPU）\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# input_ids = input_ids.to(device)\n",
    "\n",
    "# # 生成文本\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=500,  # 可以根据需要调整生成文本的长度\n",
    "#         num_return_sequences=1,\n",
    "#         pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "# # 解码并打印生成的文本\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your new prompt text here. You should have found our new search form.\n",
      "\n",
      "Hey, your search form is here.\n",
      "\n",
      "Let me ask you another question. Why?\n",
      "\n",
      "Why didn’t you just call the guy today?\n",
      "\n",
      "Well,’s what I’ve always wanted to know is, why didn’t you get out of the car after about 10 hours? Did he ask you one question?’\n",
      "\n",
      "’’Well\n",
      "Your new prompt text here. You can always view it anywhere. So when you type it, it always comes from the top. It doesn’t come from the bottom. It comes from every note you write. And then, no matter what you write, you’ll find this: “This.””””””””””””””””””\n",
      "Your new prompt text here. I don’t want you here, but we don’t want to know what that means. And so I get a little bit of a feeling that if I just look at something that’s very particular to me, like a small area in a networked network, it’s going to give you something in return.\n",
      "\n",
      "Cringeley: How do you feel about calling yourself a Ponzi scheme investor?\n",
      "\n",
      "Jobs\n",
      "Your new prompt text here. Now it says, using Microsoft Office, which means, you can take your computer anywhere you want. And that means, you can put anything you want on it. And I think that when you learn how to use Microsoft Office, it really becomes the same thing as a mouse, especially when you get to know the keyboard. And it is a really powerful tool if you pay attention. It is very hard to get through a program, and it is so much easier to\n",
      "Your new prompt text here. Just look at the top right corner. And when you look at that, it says, 'My name is Steve Jobs. I was a senior vice president at Hewlett-Packard Computer Co. And I worked for Apple. So I was a major investor in Hewlett-Packard computer company.' And you could see in this screen, that he was the one that had money in him. And he was like, Oh, that was great. I really don\n"
     ]
    }
   ],
   "source": [
    "# 准备生成文本的初始提示（prompt）\n",
    "prompt_text = \"Your new prompt text here.\"  # 更改这里的文本\n",
    "\n",
    "# 编码输入文本，添加结束标记\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# 将模型转移到正确的计算设备（GPU或CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 生成文本\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,  # 可以根据需要调整生成文本的长度\n",
    "        num_return_sequences=5,  # 生成5个句子\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,  # 开启采样策略\n",
    "        top_k=0,  # 使用全局采样\n",
    "        temperature=0.7  # 降低采样温度，使输出更加确定\n",
    "    )\n",
    "\n",
    "# 解码并打印生成的文本\n",
    "for i in range(5):  # 打印5个句子\n",
    "    generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
