{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9928e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\njjia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 12/12 [00:01<00:00,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1.3678, 'train_samples_per_second': 35.092, 'train_steps_per_second': 8.773, 'train_loss': 5.429766337076823, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=5.429766337076823, metrics={'train_runtime': 1.3678, 'train_samples_per_second': 35.092, 'train_steps_per_second': 8.773, 'train_loss': 5.429766337076823, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Read file content\n",
    "with open(\"./Copy-Cat-Project-main/Steve Jobs at Apple’s WWDC 1997.txt\", 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize words\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# Reassemble the processed text into a string\n",
    "filtered_text = ' '.join(filtered_text)\n",
    "\n",
    "# Write the processed text into a new file\n",
    "with open(\"./filtered_Steve_Jobs_Speech.txt\", 'w') as file:\n",
    "    file.write(filtered_text)\n",
    "\n",
    "#---------------------------------------------\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare dataset and data collator\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"./filtered_Steve_Jobs_Speech.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to live five hundred more years. I want to know how I got it,\" he said. \"I want to know how I made this life work. I want to know how I got my brain to work with the rules. I want to know how I got them to work. I want to know how I managed to fail in life. I want to know how I got lucky.\"\n",
      "\n",
      "What happened next was little more than the beginning of a roller coaster ride. A couple years later, he was diagnosed with Parkinson's disease and died in 1965.\n",
      "\n",
      "\"What really got him through it was when I was diagnosed with Parkinson's disease,\" he said. \"He was probably one of the longest-lived people my family had ever met. I had a dream. I would love to work with anyone with Parkinson's. I'd love to work with anybody with Alzheimer's disease, but I would love to work with anyone with Alzheimer's disease. I would love to work with\n",
      "I want to live five hundred more years. I want to die, I want to be buried, I want to be married, I want to die my last day. I want to live another ten thousand years. I want to live a thousand years. I want to see the world. I want to be an architect. I want to be a scientist. I want to develop technologies and even create inventions. And I want to live more than ten thousand years, and then I will be dead. I will die.\"\n",
      "\n",
      "It was a life worth living.\n",
      "\n",
      "I asked him where he worked and he said he worked at the British Museum in London, where he worked on film and television.\n",
      "\n",
      "\"I worked on the BBC for a decade. I worked for the Daily Mail for two years. I worked for the Daily Telegraph for a decade — I worked for the Daily Express for four years. So I got the job. And I was working for the Sunday Times for three years\n",
      "I want to live five hundred more years.\n",
      "\n",
      "My hope is not just that I'll get married someday, but that I'll live in a rock garden.\n",
      "\n",
      "Then I'll have a little baby girl.\n",
      "\n",
      "I'll grow up happy, sharp and smart.\n",
      "\n",
      "I'll be growing up a good, confident, healthy, connected, focused, successful person.\n",
      "\n",
      "I'll make the best decisions I can make.\n",
      "\n",
      "And I'll start getting on with my life.\n",
      "\n",
      "And that's when I'll know that I want to get married.\n",
      "\n",
      "That I want to live for five hundred more years and get married.\n",
      "\n",
      "That I want to be an American citizen.\n",
      "\n",
      "I'll be an American citizen in my mid-50s.\n",
      "\n",
      "But I'm going to marry a guy who wants to make the best decisions I can make.\n",
      "\n",
      "I'll get married.\n",
      "\n",
      "One day I'll go to work and I'll say\n",
      "I want to live five hundred more years. I want to live a thousand. I want to live one hundred or ten thousand years. I want to live fifty thousand or ninety thousand years. So I'm thinking about just what it means to live ten hundred and fifty thousand years. I think about three hundred thousand years, five hundred thousand years. I think about five hundred thousand years, five hundred thousand years, five hundred thousand years. I want to live twenty million years. I want to live a hundred million years. I want to live ten million years. I want to live ten million years. I want to live ten million years. I want to live ten million years. I want to live twenty million years. I want to live fifty million years. I want to live fifty million years. I want to live fifty million years. I want to live fifty million years.\"\n",
      "\n",
      "\"So that's what the salad is, right?\"\n",
      "\n",
      "\"Five hundred million years. So that's\n",
      "I want to live five hundred more years. And that's not going to happen. And I wish I had done it 30 years ago. I wish I had done it 30 years ago. I wish I had put this investment in someplace where I could figure out a way to get my whole life back together. I mean, I know I'm probably lucky enough to be alive today, but I have to get back together. I know I have to get back together. I have to get back together, and get back together. And I'm going to do this all my life, because I want to get back together, I want to get back together, and I need to get back together. I need to get back together. I need to get back together. I don't want to ever walk away from this. I want to get back together. And I want to go back. And I need to get back together. I see a lot of people want to get back together,\n",
      "I want to live five hundred more years. I want to live five hundred years without ever having to go back.\"\n",
      "\n",
      "He said: \"I'll go back to school, I'll go back to work, I'll work on it, I'll go back to my country life. I can't regret that. I can't say that I'm so happy. I know I'm no longer depressed about my life.\"\n",
      "\n",
      "Asked if he feels lucky to be alive, he replied: \"Yeah, I did. I felt really lucky to live. I'm lucky that I had the time to do that. I'm lucky to have my family back to me. I don't think I'll regret anything else.\"\n",
      "\n",
      "His mother, who is 81, died in 1996, aged 84.\n",
      "\n",
      "At the time, he said: \"It was just a great time.\"\n",
      "\n",
      "The family decided to take a short leave of absence to look after their powerful young daughter, who could\n",
      "I want to live five hundred more years. So I'm going to get married. I want to be a mother.\"\n",
      "\n",
      "She spoke with pinpoint accuracy and clarity about her life choices. She kept her handwriting clean. She was a grandmother and her mother had been a painter since she was seven years old. If she could get away with anything in life, she would get away with it.\n",
      "\n",
      "She understood her mother would always have her. She would forget about her success. She would forget about her mother. She would never forget her mother, her mother would never forget her mother. She would never forget her mother. She would never forget her mother.\n",
      "\n",
      "Every day, she would recall her mom's death. Every day, she would remember her mother's death. Every day, she would remember her mother's death. Every day, she would tell her mother that she needed to move on.\n",
      "\n",
      "She would tell her mother that her parents died so she could begin life.\n",
      "I want to live five hundred more years. I want to live 10 thousand years. I want to live 10 years where everybody knows everybody and nobody cares about anybody else. The world is the best part of the world in the way that I live it. I'm a human being. I'm a human being. So it's like living in a dream, you get in a dream you do, you live it, you rationalize it. You get in a dream you make it, you think, 'Why am I doing this?' And you go, 'This is the best job I've ever created.'\n",
      "\n",
      "Do you want to be a doctor?\n",
      "\n",
      "I want to be a doctor. I want to live my life. I want to live my life. I want to live my life. If I want to die, I want to die. I want to go live my life. But I want to live my life. I want to live my life. But that's\n",
      "I want to live five hundred more years. I want to be able to go on to have a great life, to start studying medicine and develop my own career. I want to get lucky and get a little lucky and develop my own career. And I want to get lucky and thrive and get lucky. I want to make a difference.\"\n",
      "\n",
      "The booming careers of young people in Silicon Valley started in earnest six months ago when Mark Zuckerberg announced that he was launching Facebook. After spending much of his life in Silicon Valley, Zuckerberg had a life, and next, he decided to help change the world. The idea became a part of a lifestyle he'd always dreamed of.\n",
      "\n",
      "It wasn't until five years ago that Mark Zuckerberg decided to build a company that would drive innovation, create a better world, and build a positive technology future. Facebook's founders, Sergey Brin and Sergey Brin, had teamed up to create the first Facebook Business Card, and that was it.\n",
      "\n",
      "So\n",
      "I want to live five hundred more years. I want to live five hundred more years. I want to live five hundred more years. I want to live five hundred more years. I want to live five hundred more years. I want to live three hundred years. I want to live three hundred more. I want to live three hundred fifty years. I want to live three hundred sixty years. I want to live three hundred sixty years. This should be a thought I have never thought of. I should live three hundred eighty years. I should live three hundred eighty years. I should live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three hundred eighty years. I want to live three\n"
     ]
    }
   ],
   "source": [
    "# Prepare the initial prompt for generating text\n",
    "# prompt_text = \"Your new prompt text here.\"  # Change the text here\n",
    "prompt_text = \"I want to live five hundred more years.\"\n",
    "\n",
    "# Encode input text and add end-of-sequence token\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# Move the model to the correct computing device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Generate text\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,  # Adjust the length of the generated text as needed\n",
    "        num_return_sequences=10,  # Generate 10 sentences\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,  # Enable sampling strategy\n",
    "        top_k=0,  # Use global sampling\n",
    "        temperature=0.7  # Lower the sampling temperature to make the output more deterministic\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text\n",
    "for i in range(10):  # Print 10 sentences\n",
    "    generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "with open('./results.txt', 'w', encoding='utf-8') as file:\n",
    "    for i in range(10):\n",
    "        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "        file.write(generated_text + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\njjia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.06105978863487787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1.792, 'train_samples_per_second': 33.482, 'train_steps_per_second': 8.929, 'train_loss': 1.6935263872146606, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 30.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.6799107994655182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# This is your generated text\n",
    "# generated_text = \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# Read the generated text from results.txt\n",
    "with open('./results.txt', 'r', encoding='utf-8') as file:\n",
    "    generated_text = file.read()\n",
    "\n",
    "# # This is your reference text\n",
    "# reference_texts = [\n",
    "#     \"Your reference text here.\"  # Change the text here\n",
    "# ]\n",
    "\n",
    "# Read the reference text\n",
    "with open('./Copy-Cat-Project-main/Steve Jobs at Apple’s WWDC 1997.txt', 'r') as f:\n",
    "    reference_texts = [line.strip() for line in f]\n",
    "\n",
    "# Tokenize the generated and reference texts\n",
    "tokenized_generated_text = word_tokenize(generated_text)\n",
    "tokenized_reference_texts = [word_tokenize(ref) for ref in reference_texts]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = sentence_bleu(tokenized_reference_texts, tokenized_generated_text)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "\n",
    "# Calculate Perplexity\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare the dataset and data collator\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"./results.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_dataset = TextDataset(tokenizer=tokenizer, file_path=\"./results.txt\", block_size=128)\n",
    "\n",
    "# Set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,  # Add this line\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Calculate Perplexity\n",
    "print(\"Perplexity:\", math.exp(trainer.evaluate()[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
