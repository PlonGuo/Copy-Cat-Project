{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9928e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271abbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model.The first time I saw the new \"The Walking Dead\" trailer, I was so excited. I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # 编码输入文本，添加结束标记\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    # 生成响应\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # 解码并返回响应\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 示例对话\n",
    "user_input = \"Hello, I'm a language model.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've got to find what you love, and that is as true for work as it is for your lovers.The U.S. Department of Justice has filed a lawsuit against the company that owns the National Security Agency's (NSA) Tailored Access Operations (TAO) program, alleging that the program violates the Fourth Amendment.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Check if a CUDA device is available, and if so, move the model to the CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # Encode the input text and add an end-of-sequence token\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # If a CUDA device is available, move the input data to the CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example dialogue\n",
    "user_input = \"You've got to find what you love, and that is as true for work as it is for your lovers.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Prepare Data\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "file_path = \"./Copy-Cat-Project-main/Steve Jobs Stanford Commencement Speech Transcript 2005.txt\"\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=file_path,\n",
    "    block_size=128,\n",
    "    overwrite_cache=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Training Configuration\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.1580843925476074\n",
      "Epoch: 0, Loss: 3.5734238624572754\n",
      "Epoch: 0, Loss: 3.2003633975982666\n",
      "Epoch: 0, Loss: 2.9473118782043457\n",
      "Epoch: 0, Loss: 3.428860902786255\n",
      "Epoch: 0, Loss: 3.517279624938965\n",
      "Epoch: 1, Loss: 2.60526442527771\n",
      "Epoch: 1, Loss: 3.181273937225342\n",
      "Epoch: 1, Loss: 2.845142364501953\n",
      "Epoch: 1, Loss: 2.6161742210388184\n",
      "Epoch: 1, Loss: 3.148547649383545\n",
      "Epoch: 1, Loss: 2.854081392288208\n",
      "Epoch: 2, Loss: 2.3623342514038086\n",
      "Epoch: 2, Loss: 2.8888163566589355\n",
      "Epoch: 2, Loss: 2.6013753414154053\n",
      "Epoch: 2, Loss: 2.3753368854522705\n",
      "Epoch: 2, Loss: 2.860568046569824\n",
      "Epoch: 2, Loss: 2.329719066619873\n"
     ]
    }
   ],
   "source": [
    "# Custom Training Loop\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch[\"input_ids\"].to(device), batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Your prompt text here\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=1000)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Generated Text\n",
    "with open('./results.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "100%|██████████| 24/24 [00:02<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2.408, 'train_samples_per_second': 34.884, 'train_steps_per_second': 9.967, 'train_loss': 2.8608404795328775, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=2.8608404795328775, metrics={'train_runtime': 2.408, 'train_samples_per_second': 34.884, 'train_steps_per_second': 9.967, 'train_loss': 2.8608404795328775, 'epoch': 4.0})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#更新后！！！！！\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 准备数据集和数据整理器\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=\"./Copy-Cat-Project-main/Steve Jobs Stanford Commencement Speech Transcript 2005.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# # 准备生成文本的初始提示（prompt）\n",
    "# prompt_text = \"Your prompt text here.\"\n",
    "\n",
    "# # 编码输入文本，添加结束标记\n",
    "# input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# # 将模型转移到正确的计算设备（GPU或CPU）\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# input_ids = input_ids.to(device)\n",
    "\n",
    "# # 生成文本\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=500,  # 可以根据需要调整生成文本的长度\n",
    "#         num_return_sequences=1,\n",
    "#         pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "# # 解码并打印生成的文本\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to live five hundred more years. If I could only have a three-bedroom house, I would have bought it. My mother and I bought it for $100,000. My father bought it for $30,000. My brother bought it for $85,000. It was bought in the middle of 2008 for about $16,000. My father got the deal because he wanted to give it a try, and I wanted to do everything right. Everything was fine. I would turn it into a small business and run it, but I would always strongly discourage anyone who wanted to go into business. It would be better to die than to live that way. My parents had been living around the block for eight or nine years before I was able to find a job, and I never really thought about it. I wanted to live 10 years, and then I didn't want to live that way. I came up with the idea of self-sufficiency. I had a\n",
      "I want to live five hundred more years. I want to live five hundred more years longer than I did before. And that's all I'm saying. I just want to say that I'm still here. I want to come back and live the rest of my life, and that's what I'm going to do. I want to tell my story. I want to tell it in a way that I know will help you be successful. I want to tell it in a way that I know will help you be successful. I want to tell it in a way that will inspire you to begin the journey of your life. I want to tell it in a way that will inspire you to tell the next chapter in your life. I want to tell it in a way that will inspire you to put your heart and your soul into something amazing. I want to tell it in a way that will inspire you to see the light in the darkness. I want to tell it in a way that will\n",
      "I want to live five hundred more years. How are you going to do it? I want to live five hundred more years. How are you going to do it? I want to live five hundred more years.\"\n",
      "\n",
      "Brook is an actor whose work has been recognized with many awards. He's known for his performance on the 1996 TV series I'm Your Man and was nominated for a Golden Globe for his performance as Gary Cooper on the 1960s sitcom The Big Short. He did a few movies under the name of Phil Spector before moving on to the film business. He's currently performing as a character in the film The Last Night of the Living Dead, which opens in theaters this September. He also stars as the final boss of a dysfunctional family.\n",
      "I want to live five hundred more years. If it wasn't for the way that I used to work, I would have never met my wife.\n",
      "\n",
      "\"Fortunately, I was the lucky girl who graduated from college with a degree in psychology. I was the first person in my family to graduate with a degree in sociology, and by the time my wife was pregnant, I had already graduated from college with an Ivy League degree. My mother, who was a doctor, was a very talented woman. She had been a very successful psychologist, and she was very interested in what I wanted to do. She told me that she wanted me to take a graduate degree in psychology because it would give me the opportunity to take a class in psychology. I was initially reluctant to take the class because the professors told me that the only thing I wanted to do was get a job. But I was so excited, and I got an offer.\n",
      "\n",
      "\"I was living the life I wanted to live.\n",
      "I want to live five hundred more years. I want to live to the present day. I want to live to the present day. I want to live to the future. I want to live to the future. It's all about relationships, about how you connect, what you do for a living. It's all about that. It's all about relationships. I want to live to the present day, and that's what I want to be doing. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships. It's all about relationships.\n",
      "I want to live five hundred more years. I want to be a hundred more. I want to live more like my grandmother, by the way. I want to live more like my mother, by the way.\"\n",
      "\n",
      "This is the kind of life that any architect, as well as any architect, needs to have. It is very hard to live as long as you can. This is huge for every architect, but it is also easy to be successful in any field. So how do you make it possible for yourself to become successful in the field you love?\n",
      "\n",
      "I'm not going to say how I made it, but I went in with a long list of very specific and very clear goals. These were the ones that I had to tell myself. If I didn't know how to make it, I had to start over. I had to know if I was going to have a career, if I wanted to be a doctor, or if I wanted to do something that would\n",
      "I want to live five hundred more years. I want to live five hundred more years. I want to live five hundred more years. I can't do it. I don't know what to do. I'm not going to do it.\"\n",
      "\n",
      "The previous year, he had a big heart attack, and he got sick, and he took his own life. The last thing he did was carry on with his career. Then, while working as a security guard at Apple, Apple changed his mind about starting at Google, and he decided to quit. He worked at Google for seven months, and he decided to leave. He had four family members, and he was already doing very well. But then, the company stopped paying him, and he decided to start his own company, Google Docs.\n",
      "\n",
      "\"I was very lucky to have a great manager who had been around for a long time,\" says Shaun. \"I worked at Google for almost two years. I was fortunate enough\n",
      "I want to live five hundred more years. I want to build a great, great city. I want to start the next big thing. None of this is going to happen by accident. I'm going to have to do it because I want to be the best I can be. I just don't want to do it by choice. I want to do it because I want to live a better life than any of the other people on the planet today.\"\n",
      "\n",
      "Of course, the success of his career wasn't just about him. It was about the people he became famous for. The success of his career was about the people he adopted. When I was in college, I had a class where all the class kids were going to college, and they had all the money they could afford, and they all went to college. It was a very exciting time. I had a love affair with that love affair. It came from the very beginning and I loved it. It was a new beginning\n",
      "I want to live five hundred more years.\n",
      "\n",
      "The only way I'm going to die is if I die young. When I was younger, I always thought it was a good idea to die young. But then I realized that it's not a good idea to live long enough to get by. That was when I found out that I would be 100 percent healthy and I would need to be on the move for seven months before I could get out. That was the beginning of my recovery. I didn't want to give up, so I decided to give it a try. I thought I'd give it a try and see what it would do. I started with a plan of my own, and it worked out pretty well. I wrote down the details of my life and everything I'd learned, including how I learned to stop worrying about my body. It was the first time I actually had a plan in place. I finished the plan, and my doctors told me I was going to\n",
      "I want to live five hundred more years. I want to be a doctor. And I want to do it with the help of a partner who is a great, wonderful, wonderful person who can make it all work.\"\n",
      "\n",
      "Abjurer was diagnosed as having a rare form of nervous system cancer, but it was too late. In 1966, his wife, Gwen, passed away, leaving Abjurer with only an unopened envelope with the words \"Father, my love.\" After his death, Abjurer tried to find a way to raise money for his wife, but as a last resort, he started a new life as a photo editor at the New York Times, where he would do the same thing he did writing in his spare time.\n",
      "\n",
      "Abjurer died in 1970, just as the New York Times was closing on a merger with Time, and his daughter, Mary, was a virgin. Mary was born on April 10, 1971, the same day Abj\n"
     ]
    }
   ],
   "source": [
    "# 准备生成文本的初始提示（prompt）\n",
    "# prompt_text = \"Your new prompt text here.\"  # 更改这里的文本\n",
    "prompt_text = \"I want to live five hundred more years.\"\n",
    "\n",
    "# 编码输入文本，添加结束标记\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "\n",
    "# 将模型转移到正确的计算设备（GPU或CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# 生成文本\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,  # 可以根据需要调整生成文本的长度\n",
    "        num_return_sequences=10,  # 生成5个句子\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,  # 开启采样策略\n",
    "        top_k=0,  # 使用全局采样\n",
    "        temperature=0.7  # 降低采样温度，使输出更加确定\n",
    "    )\n",
    "\n",
    "# 解码并打印生成的文本\n",
    "for i in range(10):  # 打印5个句子\n",
    "    generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n",
    "#将生成的文本保存到文件中\n",
    "with open('./results.txt', 'w', encoding='utf-8') as file:\n",
    "    for i in range(10):\n",
    "        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "        file.write(generated_text + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.07712489872555123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\njjia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 这是你的生成文本\n",
    "# generated_text = \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "#生成的文本从results.txt中读取\n",
    "with open('./results.txt', 'r', encoding='utf-8') as file:\n",
    "    generated_text = file.read()\n",
    "\n",
    "# # 这是你的参考文本\n",
    "# reference_texts = [\n",
    "#     \"Your reference text here.\"  # 更改这里的文本\n",
    "# ]\n",
    "\n",
    "# 读取参考文本\n",
    "with open('./Copy-Cat-Project-main/Steve Jobs Stanford Commencement Speech Transcript 2005.txt', 'r') as f:\n",
    "    reference_texts = [line.strip() for line in f]\n",
    "\n",
    "# 对生成的文本和参考文本进行分词\n",
    "tokenized_generated_text = word_tokenize(generated_text)\n",
    "tokenized_reference_texts = [word_tokenize(ref) for ref in reference_texts]\n",
    "\n",
    "# 计算 BLEU 分数\n",
    "bleu_score = sentence_bleu(tokenized_reference_texts, tokenized_generated_text)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
