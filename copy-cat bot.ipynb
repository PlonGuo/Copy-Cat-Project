{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9928e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271abbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm a language model.The first time I saw the new \"The Walking Dead\" trailer, I was so excited. I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see the first trailer for the show, and I was so excited to see\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # Encode the input text and add a closing tag\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "    \n",
    "    # Render Response\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode and return a response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Dialogues\n",
    "user_input = \"Hello, I'm a language model.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've got to find what you love, and that is as true for work as it is for your lovers.The U.S. Department of Justice has filed a lawsuit against the company that owns the National Security Agency's (NSA) Tailored Access Operations (TAO) program, alleging that the program violates the Fourth Amendment.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring the NSA to obtain a warrant before it can access the data of millions of Americans.\n",
      "\n",
      "The lawsuit, filed in U.S. District Court in Washington, D.C., alleges that the NSA's Tailored Access Operations (TAO) program violates the Fourth Amendment by requiring\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Check if a CUDA device is available, and if so, move the model to the CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # Encode the input text and add an end-of-sequence token\n",
    "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # If a CUDA device is available, move the input data to the CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.zto('cuda')\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example dialogue\n",
    "user_input = \"You've got to find what you love, and that is as true for work as it is for your lovers.\"\n",
    "response = generate_response(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  Prepare Data\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "file_path = \"./Copy-Cat-Project-main/Steve Jobs Stanford Commencement Speech Transcript 2005.txt\"\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=file_path,\n",
    "    block_size=128,\n",
    "    overwrite_cache=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\njjia\\anaconda3\\envs\\virtualEnv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Training Configuration\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.1580843925476074\n",
      "Epoch: 0, Loss: 3.5734238624572754\n",
      "Epoch: 0, Loss: 3.2003633975982666\n",
      "Epoch: 0, Loss: 2.9473118782043457\n",
      "Epoch: 0, Loss: 3.428860902786255\n",
      "Epoch: 0, Loss: 3.517279624938965\n",
      "Epoch: 1, Loss: 2.60526442527771\n",
      "Epoch: 1, Loss: 3.181273937225342\n",
      "Epoch: 1, Loss: 2.845142364501953\n",
      "Epoch: 1, Loss: 2.6161742210388184\n",
      "Epoch: 1, Loss: 3.148547649383545\n",
      "Epoch: 1, Loss: 2.854081392288208\n",
      "Epoch: 2, Loss: 2.3623342514038086\n",
      "Epoch: 2, Loss: 2.8888163566589355\n",
      "Epoch: 2, Loss: 2.6013753414154053\n",
      "Epoch: 2, Loss: 2.3753368854522705\n",
      "Epoch: 2, Loss: 2.860568046569824\n",
      "Epoch: 2, Loss: 2.329719066619873\n"
     ]
    }
   ],
   "source": [
    "# Custom Training Loop\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs, labels = batch[\"input_ids\"].to(device), batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Text Generation\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Your prompt text here\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=1000)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Generated Text\n",
    "with open('./results.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualEnv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
